{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "模型从数据中学习：模型的训练过程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 损失函数\n",
    "def mean_squared_error(y, t):\n",
    "    return 0.5 * np.sum((y-t)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09750000000000003"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 一个计算的例子\n",
    "# 设“2”为正确解\n",
    "t = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
    "# 例1：“2”的概率最高的情况（0.6）\n",
    "y = [0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0]\n",
    "mean_squared_error(np.array(y), np.array(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5975"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 例2：“7”的概率最高的情况（0.6）\n",
    "y = [0.1, 0.05, 0.1, 0.0, 0.05, 0.1, 0.0, 0.6, 0.0, 0.0]\n",
    "mean_squared_error(np.array(y), np.array(t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "交叉熵损失函数\n",
    "\n",
    "$E=-\\sum_k{t_k log y_k}$\n",
    "\n",
    "$y_k$是神经网络的输出，$t_k&是正确解标签。并且，$t_k$ 中只有正确解标签的索引为1，其他均为0（one-hot表示）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 交叉熵损失函数\n",
    "def cross_entropy_error(y, t):\n",
    "    delta = 1e-7\n",
    "    return -np.sum(t * np.log(y + delta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.510825457099338"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 计算实例\n",
    "t = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
    "y = [0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0]\n",
    "cross_entropy_error(np.array(y), np.array(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.302584092994546"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = [0.1, 0.05, 0.1, 0.0, 0.05, 0.1, 0.0, 0.6, 0.0, 0.0]\n",
    "cross_entropy_error(np.array(y), np.array(t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "实际上，在学习的过程中，应当把所有训练数据的损失函数都作为目标进行学习。\n",
    "\n",
    "因此，损失函数可以进一步写为：$E=-\\frac{1}{N}\\sum_n\\sum_k t_{nk}logy_{nk}$\n",
    "\n",
    "在实际的计算过程中，我们是从训练数据中选择一批去计算损失函数的值。\n",
    "\n",
    "神经网络的学习也是从训练数据中选出一批数据（称为mini-batch,小批量），然后对每个mini-batch进行学习。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n",
      "(60000, 10)\n"
     ]
    }
   ],
   "source": [
    "# 如何设定批量？\n",
    "\n",
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "import numpy as np\n",
    "from dataset.mnist import load_mnist\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    "print(x_train.shape) # (60000, 784)\n",
    "print(t_train.shape) # (60000, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 从所有数据中随机选取10个数据\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 10\n",
    "batch_mask = np.random.choice(train_size, batch_size)\n",
    "x_batch = x_train[batch_mask]\n",
    "t_batch = t_train[batch_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 实现batch版的交叉熵\n",
    "# one-hot版本\n",
    "def cross_entropy_error(y, t):\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1, y.size)\n",
    "    batch_size = y.shape[0]\n",
    "    return -np.sum(t * np.log(y + 1e-7)) / batch_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数字标签版本\n",
    "def cross_entropy_error_new_label(y, t):\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1, y.size)\n",
    "    batch_size = y.shape[0]\n",
    "    return -np.sum(np.log(y[np.arange(batch_size), t] + 1e-7)) / batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4 5 6 7 8 9]\n"
     ]
    }
   ],
   "source": [
    "batch_size = 10\n",
    "print(np.arange(batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.62462369 0.99127311 0.58472238 0.42636286 0.80585446 0.72039222\n",
      "  0.36890134 0.00203693 0.6545725  0.51591287]\n",
      " [0.82720597 0.20737656 0.25668042 0.14391568 0.23246991 0.87753964\n",
      "  0.9407443  0.92570053 0.07383895 0.01174901]\n",
      " [0.34846777 0.42404329 0.35524991 0.3393804  0.2997245  0.25953985\n",
      "  0.02977389 0.81367863 0.44669289 0.88051307]\n",
      " [0.04953133 0.34553479 0.85132736 0.73755216 0.715663   0.39167197\n",
      "  0.58783915 0.75015796 0.09112103 0.62883957]\n",
      " [0.83206471 0.41865225 0.09756433 0.82680445 0.74473453 0.23053902\n",
      "  0.0495651  0.1187685  0.74352963 0.74681066]\n",
      " [0.558113   0.69933344 0.95408351 0.64389968 0.03163256 0.28978572\n",
      "  0.77521213 0.14535819 0.21892047 0.94270915]\n",
      " [0.5330108  0.0249642  0.09286895 0.47731829 0.67957642 0.1448772\n",
      "  0.8760873  0.25298544 0.27662369 0.28476743]\n",
      " [0.76568195 0.53752116 0.38052437 0.71966969 0.52268852 0.97669221\n",
      "  0.98841442 0.69979487 0.75150102 0.46601202]\n",
      " [0.32089282 0.95453238 0.85501764 0.85916208 0.46060409 0.02977806\n",
      "  0.58148816 0.38909345 0.23869629 0.86102681]\n",
      " [0.96690003 0.27078412 0.31664609 0.08026681 0.74329725 0.80857324\n",
      "  0.8768795  0.61623176 0.50726299 0.27148955]]\n",
      "[array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]), array([2, 7, 8, 4, 5, 3, 2, 4, 1, 8])]\n",
      "[0.58472238 0.92570053 0.44669289 0.715663   0.23053902 0.64389968\n",
      " 0.09286895 0.52268852 0.95453238 0.50726299]\n"
     ]
    }
   ],
   "source": [
    "t = [2,7,8,4,5,3,2,4,1,8]\n",
    "y = np.random.uniform(0,1,(10,10))\n",
    "l = [np.arange(batch_size),np.array(t)]\n",
    "print(y)\n",
    "print(l)\n",
    "print(y[np.arange(batch_size),np.array(t)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为什么需要损失函数？\n",
    "\n",
    "在进行神经网络的学习时，不能将识别精度作为指标。因为如果以识别精度为指标，则参数的导数在绝大多数地方都会变为0。\n",
    "\n",
    "使用阶跃函数作为激活函数存在同样的问题。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数值微分\n",
    "def numerical_diff(f, x):\n",
    "    h = 1e-4 # 0.0001\n",
    "    return (f(x+h) - f(x-h)) / (2*h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1999999999990898"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 例子\n",
    "def function_1(x):\n",
    "    return 0.01*x**2 + 0.1*x\n",
    "\n",
    "numerical_diff(function_1, 5) # 数值解为0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 求偏导的例子\n",
    "def function_2(x):\n",
    "    return x[0]**2 + x[1]**2\n",
    "    # 或者return np.sum(x**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.00000000000378"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def function_tmp1(x0):\n",
    "    return x0*x0 + 4.0**2.0\n",
    "numerical_diff(function_tmp1, 3.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "偏导数需要将多个变量中的某一个变量定为目标变量，并将其他变量固定为某个值。\n",
    "\n",
    "由全部变量的偏导数汇总而成的向量称为梯度（gradient）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 求梯度的函数\n",
    "# 注意：该函数只对一维的权重有效\n",
    "def numerical_gradient(f, x):\n",
    "    h = 1e-4 # 0.0001\n",
    "    grad = np.zeros_like(x) # 生成和x形状相同的数组\n",
    "    for idx in range(x.size):\n",
    "        tmp_val = x[idx]\n",
    "        # f(x+h)的计算\n",
    "        x[idx] = tmp_val + h\n",
    "        fxh1 = f(x)\n",
    "        # f(x-h)的计算\n",
    "        x[idx] = tmp_val - h\n",
    "        fxh2 = f(x)\n",
    "        grad[idx] = (fxh1 - fxh2) / (2*h)\n",
    "        x[idx] = tmp_val # 还原值\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "神经网络的优化过程中，所采用的方法为梯度（下降）法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 梯度下降法的简单呈现\n",
    "def gradient_descent(f, init_x, lr=0.01, step_num=100):\n",
    "    x = init_x\n",
    "    for i in range(step_num):\n",
    "        grad = numerical_gradient(f, x)\n",
    "        x -= lr * grad\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-6.11110793e-10,  8.14814391e-10])"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 利用一个普通的函数进行实验\n",
    "def function_2(x):\n",
    "    return x[0]**2 + x[1]**2\n",
    "\n",
    "init_x = np.array([-3.0, 4.0])\n",
    "gradient_descent(function_2, init_x=init_x, lr=0.1, step_num=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对一个简单的神经网络求梯度\n",
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "import numpy as np\n",
    "def numerical_gradient(f, x):\n",
    "    # 这是对多维权重也有效的数值微分函数\n",
    "    h = 1e-4 # 0.0001\n",
    "    grad = np.zeros_like(x)\n",
    "    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
    "    while not it.finished:\n",
    "        idx = it.multi_index\n",
    "        tmp_val = x[idx]\n",
    "        x[idx] = tmp_val + h\n",
    "        fxh1 = f(x) # f(x+h)\n",
    "        \n",
    "        x[idx] = tmp_val - h \n",
    "        fxh2 = f(x) # f(x-h)\n",
    "        grad[idx] = (fxh1 - fxh2) / (2*h)\n",
    "        \n",
    "        x[idx] = tmp_val # 値を元に戻す\n",
    "        it.iternext()   \n",
    "        \n",
    "    return grad\n",
    "def softmax(a):\n",
    "    c = np.max(a)\n",
    "    exp_a = np.exp(a - c) # 溢出对策\n",
    "    sum_exp_a = np.sum(exp_a,axis=-1, keepdims=True)\n",
    "    y = exp_a / sum_exp_a\n",
    "    return y\n",
    "\n",
    "class simpleNet:\n",
    "    def __init__(self):\n",
    "        self.W = np.random.randn(2,3) # 用高斯分布进行初始化\n",
    "    def predict(self, x):\n",
    "        return np.dot(x, self.W)\n",
    "    def loss(self, x, t):\n",
    "        z = self.predict(x)\n",
    "        y = softmax(z)\n",
    "        loss = cross_entropy_error(y, t)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.11618769  0.10679615  1.39532972]\n",
      " [-1.46325012 -1.1249419   0.63860369]]\n",
      "[-0.64721249 -0.94837002  1.41194115]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.2004500594137805"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = simpleNet()\n",
    "print(net.W)\n",
    "x = np.array([0.6, 0.9])\n",
    "p = net.predict(x)\n",
    "print(p)\n",
    "np.argmax(p) # 最大值的索引\n",
    "t = np.array([0, 0, 1]) # 正确解标签\n",
    "net.loss(x, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.06263509  0.04634754 -0.10898263]\n",
      " [ 0.09395264  0.06952131 -0.16347395]]\n"
     ]
    }
   ],
   "source": [
    "# 求梯度\n",
    "def f(W):\n",
    "    return net.loss(x, t)\n",
    "\n",
    "dW = numerical_gradient(f, net.W)\n",
    "print(dW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.9575853  -0.05949021  0.32177548]\n",
      " [ 0.72889458 -1.03723657  0.06966219]]\n",
      "(0, 0)\n",
      "(0, 1)\n",
      "(0, 2)\n",
      "(1, 0)\n",
      "(1, 1)\n",
      "(1, 2)\n"
     ]
    }
   ],
   "source": [
    "x = np.random.randn(2,3)\n",
    "print(x)\n",
    "it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
    "# 这是用来输出数组索引的迭代对象\n",
    "# 其中readwrite代表可以对数组对象进行修改\n",
    "while not it.finished:\n",
    "    idx = it.multi_index\n",
    "    print(idx)\n",
    "    it.iternext()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl_numpy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
